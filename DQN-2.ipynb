{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e620552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "306060d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Peice array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start SOC\n",
    "        self.state = 20 #+ random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        self.currentprice = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 sell\n",
    "        # 1 -1 = 0 hold\n",
    "        # 2 -1 = 1 buy \n",
    "        self.state += (action - 1) * self.power \n",
    "        self.currentprice = self.priceArray[self.currentIndex]\n",
    "\n",
    "         \n",
    "        if 100 < self.state < (self.max_state + self.power):\n",
    "            self.state = 100\n",
    "        elif (0 - self.power) < self.state < 0:\n",
    "            self.state = 0\n",
    "            \n",
    "        # Calculate reward    \n",
    "        # BUy\n",
    "        # Give plus reward is charge at low proce, and minus at high price\n",
    "\n",
    "        if self.state < 80:\n",
    "            self.change = -10\n",
    "        self.temp = -self.currentprice * (action - 1)\n",
    "        reward = self.temp + self.change\n",
    "\n",
    "        \n",
    "        # Move to the next price\n",
    "        self.currentIndex += 1\n",
    "        \n",
    "        # Set Low boundary and Up boundary\n",
    "        #Set end state\n",
    "        # Set Low boundary and Up boundary\n",
    "        # Set end state\n",
    "        if self.currentIndex == len(self.priceArray)-1 and self.state >= 80:\n",
    "            done = True               \n",
    "        elif self.currentIndex == len(self.priceArray)-1 and self.state < 80:\n",
    "            done = True\n",
    "            reward = -1000           \n",
    "        elif self.state < 0 or self.state > 100:\n",
    "            done = True\n",
    "            reward = -1000\n",
    "        else:\n",
    "            done = False       \n",
    "\n",
    "        \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 20 #+ random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        self.currentprice = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fbad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChargeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca77928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.755512], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "552762b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-1012.0200004577637 State:-20\n",
      "Episode:2 Score:-973.7799987792969 State:-20\n",
      "Episode:3 Score:-1067.7400000095367 State:-20\n",
      "Episode:4 Score:-1212.0899993777275 State:120\n",
      "Episode:5 Score:-973.7799987792969 State:-20\n",
      "Episode:6 Score:-1157.1300001740456 State:-20\n",
      "Episode:7 Score:-1052.7100000977516 State:-20\n",
      "Episode:8 Score:-1199.4300003647804 State:-20\n",
      "Episode:9 Score:-1208.980003118515 State:60\n",
      "Episode:10 Score:-1191.3900003433228 State:120\n",
      "Episode:11 Score:-1002.0200004577637 State:-20\n",
      "Episode:12 Score:-1066.5499987006187 State:-20\n",
      "Episode:13 Score:-1097.2400012612343 State:-20\n",
      "Episode:14 Score:-1154.2399985194206 State:120\n",
      "Episode:15 Score:-1228.6699987053871 State:120\n",
      "Episode:16 Score:-973.7799987792969 State:-20\n",
      "Episode:17 Score:-1155.2500021457672 State:-20\n",
      "Episode:18 Score:-1038.8600015640259 State:-20\n",
      "Episode:19 Score:-992.0200004577637 State:-20\n",
      "Episode:20 Score:-1163.5500014424324 State:-20\n",
      "Episode:21 Score:-1222.0799995064735 State:-20\n",
      "Episode:22 Score:-1003.7799987792969 State:-20\n",
      "Episode:23 Score:-973.7799987792969 State:-20\n",
      "Episode:24 Score:-1222.5400008559227 State:120\n",
      "Episode:25 Score:-1310.9500024318695 State:0\n",
      "Episode:26 Score:-993.7799987792969 State:-20\n",
      "Episode:27 Score:-1197.040001153946 State:-20\n",
      "Episode:28 Score:-1273.4300005435944 State:40\n",
      "Episode:29 Score:-1194.179998576641 State:120\n",
      "Episode:30 Score:-1079.7299981713295 State:-20\n",
      "Episode:31 Score:-1249.600001335144 State:120\n",
      "Episode:32 Score:-1197.6500005722046 State:120\n",
      "Episode:33 Score:-1109.9500018954277 State:-20\n",
      "Episode:34 Score:-1254.270001411438 State:60\n",
      "Episode:35 Score:-1053.9999995231628 State:-20\n",
      "Episode:36 Score:-1195.4900010228157 State:120\n",
      "Episode:37 Score:-1260.1099998950958 State:20\n",
      "Episode:38 Score:-1179.3300006389618 State:120\n",
      "Episode:39 Score:-973.7799987792969 State:-20\n",
      "Episode:40 Score:-973.7799987792969 State:-20\n",
      "Episode:41 Score:-1196.9899997115135 State:120\n",
      "Episode:42 Score:-973.7799987792969 State:-20\n",
      "Episode:43 Score:-992.0200004577637 State:-20\n",
      "Episode:44 Score:-993.7799987792969 State:-20\n",
      "Episode:45 Score:-1023.5899989008904 State:-20\n",
      "Episode:46 Score:-1030.619999885559 State:-20\n",
      "Episode:47 Score:-973.7799987792969 State:-20\n",
      "Episode:48 Score:-1045.4000000953674 State:-20\n",
      "Episode:49 Score:-1031.8300005793571 State:-20\n",
      "Episode:50 Score:-239.36000061035156 State:80\n",
      "Episode:51 Score:-1170.1799988150597 State:120\n",
      "Episode:52 Score:-327.77000242471695 State:100\n",
      "Episode:53 Score:-1150.6400010585785 State:-20\n",
      "Episode:54 Score:-1182.1199996471405 State:120\n",
      "Episode:55 Score:-1257.7799987792969 State:40\n",
      "Episode:56 Score:-1046.969998538494 State:-20\n",
      "Episode:57 Score:-1162.2999982833862 State:120\n",
      "Episode:58 Score:-1141.3599967360497 State:-20\n",
      "Episode:59 Score:-1038.8600015640259 State:-20\n",
      "Episode:60 Score:-1193.2100007534027 State:120\n",
      "Episode:61 Score:-1064.8599984645844 State:-20\n",
      "Episode:62 Score:-1289.330001592636 State:60\n",
      "Episode:63 Score:-1186.16999822855 State:120\n",
      "Episode:64 Score:-252.90000051259995 State:80\n",
      "Episode:65 Score:-279.18999963998795 State:100\n",
      "Episode:66 Score:-1054.529998242855 State:-20\n",
      "Episode:67 Score:-1082.0500013232231 State:-20\n",
      "Episode:68 Score:-267.6800032258034 State:80\n",
      "Episode:69 Score:-256.26000052690506 State:100\n",
      "Episode:70 Score:-1121.329999923706 State:-20\n",
      "Episode:71 Score:-973.7799987792969 State:-20\n",
      "Episode:72 Score:-973.7799987792969 State:-20\n",
      "Episode:73 Score:-1168.1799982190132 State:20\n",
      "Episode:74 Score:-1194.9400000572205 State:-20\n",
      "Episode:75 Score:-1345.3999996185303 State:60\n",
      "Episode:76 Score:-1025.4000000953674 State:-20\n",
      "Episode:77 Score:-1079.2799991965294 State:-20\n",
      "Episode:78 Score:-983.7799987792969 State:-20\n",
      "Episode:79 Score:-307.48000103235245 State:100\n",
      "Episode:80 Score:-1138.6699995398521 State:120\n",
      "Episode:81 Score:-973.7799987792969 State:-20\n",
      "Episode:82 Score:-1088.990002334118 State:-20\n",
      "Episode:83 Score:-1102.5700011253357 State:-20\n",
      "Episode:84 Score:-1228.3199991583824 State:-20\n",
      "Episode:85 Score:-1063.2900000214577 State:-20\n",
      "Episode:86 Score:-1284.329999923706 State:40\n",
      "Episode:87 Score:-1078.8600015640259 State:-20\n",
      "Episode:88 Score:-311.35999923944473 State:100\n",
      "Episode:89 Score:-1081.2000014781952 State:-20\n",
      "Episode:90 Score:-1200.0499997138977 State:-20\n",
      "Episode:91 Score:-1216.1700014472008 State:40\n",
      "Episode:92 Score:-973.7799987792969 State:-20\n",
      "Episode:93 Score:-1215.5999994874 State:120\n",
      "Episode:94 Score:-1280.3000015616417 State:40\n",
      "Episode:95 Score:-331.10000014305115 State:80\n",
      "Episode:96 Score:-1250.3900009989738 State:60\n",
      "Episode:97 Score:-1196.5099971294403 State:120\n",
      "Episode:98 Score:-973.7799987792969 State:-20\n",
      "Episode:99 Score:-1170.6999995708466 State:120\n",
      "Episode:100 Score:-1135.7999992966652 State:-20\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4bc362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4592ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2eabdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c197b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply when 'Sequential' object has no attribute '_compile_time_distribution_strategy'\n",
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ce45bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "558918c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 24)                48        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "feafaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfd500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 92s 9ms/step - reward: -81.5540\n",
      "758 episodes - episode_reward: -1075.892 [-1284.200, -260.070] - loss: 46435.200 - mse: 31706.523 - mean_q: -14.587\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -52.4892\n",
      "435 episodes - episode_reward: -1206.679 [-1242.660, -1158.250] - loss: 26660.461 - mse: 27079.703 - mean_q: -87.068\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -58.7703\n",
      "496 episodes - episode_reward: -1184.827 [-1252.100, -973.780] - loss: 11714.465 - mse: 106823.898 - mean_q: -347.106\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -111.7655\n",
      "1026 episodes - episode_reward: -1089.360 [-1318.760, -272.030] - loss: 8976.312 - mse: 164480.594 - mean_q: -383.066\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 90s 9ms/step - reward: -56.0157\n",
      "done, took 449.419 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f802fad30>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4), metrics=['mse']) #mean squared error mse, mean absloute error mae.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19df7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -1217.000, steps: 23\n",
      "Episode 2: reward: -1217.000, steps: 23\n",
      "Episode 3: reward: -1217.000, steps: 23\n",
      "Episode 4: reward: -1217.000, steps: 23\n",
      "Episode 5: reward: -1217.000, steps: 23\n",
      "Episode 6: reward: -1217.000, steps: 23\n",
      "Episode 7: reward: -1217.000, steps: 23\n",
      "Episode 8: reward: -1217.000, steps: 23\n",
      "Episode 9: reward: -1217.000, steps: 23\n",
      "Episode 10: reward: -1217.000, steps: 23\n",
      "Episode 11: reward: -1217.000, steps: 23\n",
      "Episode 12: reward: -1217.000, steps: 23\n",
      "Episode 13: reward: -1217.000, steps: 23\n",
      "Episode 14: reward: -1217.000, steps: 23\n",
      "Episode 15: reward: -1217.000, steps: 23\n",
      "Episode 16: reward: -1217.000, steps: 23\n",
      "Episode 17: reward: -1217.000, steps: 23\n",
      "Episode 18: reward: -1217.000, steps: 23\n",
      "Episode 19: reward: -1217.000, steps: 23\n",
      "Episode 20: reward: -1217.000, steps: 23\n",
      "Episode 21: reward: -1217.000, steps: 23\n",
      "Episode 22: reward: -1217.000, steps: 23\n",
      "Episode 23: reward: -1217.000, steps: 23\n",
      "Episode 24: reward: -1217.000, steps: 23\n",
      "Episode 25: reward: -1217.000, steps: 23\n",
      "Episode 26: reward: -1217.000, steps: 23\n",
      "Episode 27: reward: -1217.000, steps: 23\n",
      "Episode 28: reward: -1217.000, steps: 23\n",
      "Episode 29: reward: -1217.000, steps: 23\n",
      "Episode 30: reward: -1217.000, steps: 23\n",
      "Episode 31: reward: -1217.000, steps: 23\n",
      "Episode 32: reward: -1217.000, steps: 23\n",
      "Episode 33: reward: -1217.000, steps: 23\n",
      "Episode 34: reward: -1217.000, steps: 23\n",
      "Episode 35: reward: -1217.000, steps: 23\n",
      "Episode 36: reward: -1217.000, steps: 23\n",
      "Episode 37: reward: -1217.000, steps: 23\n",
      "Episode 38: reward: -1217.000, steps: 23\n",
      "Episode 39: reward: -1217.000, steps: 23\n",
      "Episode 40: reward: -1217.000, steps: 23\n",
      "Episode 41: reward: -1217.000, steps: 23\n",
      "Episode 42: reward: -1217.000, steps: 23\n",
      "Episode 43: reward: -1217.000, steps: 23\n",
      "Episode 44: reward: -1217.000, steps: 23\n",
      "Episode 45: reward: -1217.000, steps: 23\n",
      "Episode 46: reward: -1217.000, steps: 23\n",
      "Episode 47: reward: -1217.000, steps: 23\n",
      "Episode 48: reward: -1217.000, steps: 23\n",
      "Episode 49: reward: -1217.000, steps: 23\n",
      "Episode 50: reward: -1217.000, steps: 23\n",
      "Episode 51: reward: -1217.000, steps: 23\n",
      "Episode 52: reward: -1217.000, steps: 23\n",
      "Episode 53: reward: -1217.000, steps: 23\n",
      "Episode 54: reward: -1217.000, steps: 23\n",
      "Episode 55: reward: -1217.000, steps: 23\n",
      "Episode 56: reward: -1217.000, steps: 23\n",
      "Episode 57: reward: -1217.000, steps: 23\n",
      "Episode 58: reward: -1217.000, steps: 23\n",
      "Episode 59: reward: -1217.000, steps: 23\n",
      "Episode 60: reward: -1217.000, steps: 23\n",
      "Episode 61: reward: -1217.000, steps: 23\n",
      "Episode 62: reward: -1217.000, steps: 23\n",
      "Episode 63: reward: -1217.000, steps: 23\n",
      "Episode 64: reward: -1217.000, steps: 23\n",
      "Episode 65: reward: -1217.000, steps: 23\n",
      "Episode 66: reward: -1217.000, steps: 23\n",
      "Episode 67: reward: -1217.000, steps: 23\n",
      "Episode 68: reward: -1217.000, steps: 23\n",
      "Episode 69: reward: -1217.000, steps: 23\n",
      "Episode 70: reward: -1217.000, steps: 23\n",
      "Episode 71: reward: -1217.000, steps: 23\n",
      "Episode 72: reward: -1217.000, steps: 23\n",
      "Episode 73: reward: -1217.000, steps: 23\n",
      "Episode 74: reward: -1217.000, steps: 23\n",
      "Episode 75: reward: -1217.000, steps: 23\n",
      "Episode 76: reward: -1217.000, steps: 23\n",
      "Episode 77: reward: -1217.000, steps: 23\n",
      "Episode 78: reward: -1217.000, steps: 23\n",
      "Episode 79: reward: -1217.000, steps: 23\n",
      "Episode 80: reward: -1217.000, steps: 23\n",
      "Episode 81: reward: -1217.000, steps: 23\n",
      "Episode 82: reward: -1217.000, steps: 23\n",
      "Episode 83: reward: -1217.000, steps: 23\n",
      "Episode 84: reward: -1217.000, steps: 23\n",
      "Episode 85: reward: -1217.000, steps: 23\n",
      "Episode 86: reward: -1217.000, steps: 23\n",
      "Episode 87: reward: -1217.000, steps: 23\n",
      "Episode 88: reward: -1217.000, steps: 23\n",
      "Episode 89: reward: -1217.000, steps: 23\n",
      "Episode 90: reward: -1217.000, steps: 23\n",
      "Episode 91: reward: -1217.000, steps: 23\n",
      "Episode 92: reward: -1217.000, steps: 23\n",
      "Episode 93: reward: -1217.000, steps: 23\n",
      "Episode 94: reward: -1217.000, steps: 23\n",
      "Episode 95: reward: -1217.000, steps: 23\n",
      "Episode 96: reward: -1217.000, steps: 23\n",
      "Episode 97: reward: -1217.000, steps: 23\n",
      "Episode 98: reward: -1217.000, steps: 23\n",
      "Episode 99: reward: -1217.000, steps: 23\n",
      "Episode 100: reward: -1217.000, steps: 23\n",
      "-1217.0000000596046\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f826195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
