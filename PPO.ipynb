{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9e8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f5611f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 53, 141, 231],\n",
       "       [ 95,  21,  44],\n",
       "       [  9, 171, 159]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(0,255,shape=(3,3), dtype=int).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57340f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Peice array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start SOC\n",
    "        self.state = 20 + random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        self.currentprice = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        self.open = 0\n",
    "        # Set total time \n",
    "        self.time = 23 \n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 sell\n",
    "        # 1 -1 = 0 hold\n",
    "        # 2 -1 = 1 buy \n",
    "        # Update the state\n",
    "        self.state += (action - 1) * self.power  \n",
    "        \n",
    "        self.currentprice = self.priceArray[self.currentIndex]\n",
    "        \n",
    "        # Reduce time by 1 h\n",
    "        self.time -= 1 \n",
    "        \n",
    "        # If state exceeds boundaries after the action due to power limit, set state to 100 or 0        \n",
    "        if 100 < self.state < (self.max_state + self.power):\n",
    "            self.state = 100\n",
    "        if (0 - self.power) < self.state < 0:\n",
    "            self.state = 0\n",
    "            \n",
    "        # Calculate reward    \n",
    "        # BUy\n",
    "       # Give plus reward when charge at low proce, and minus at high price, and 0 at hold\n",
    "        if self.currentprice < (self.mid - 2):\n",
    "            temp = abs(self.currentprice) * (action - 1)\n",
    "            reward = temp\n",
    "            \n",
    "        # Sell\n",
    "        # Give minus reward when discharge at low proce, and plus at high price, and 0 at hold\n",
    "        elif self.currentprice > (self.mid + 2):\n",
    "            temp = -abs(self.currentprice) * (action - 1)\n",
    "            reward = temp\n",
    "            \n",
    "        # Hold\n",
    "        # Give plus reward when hold in median price range, and 0 reward otherwise\n",
    "        else:\n",
    "            temp = abs(abs(action) - 1) * self.mid\n",
    "            reward = temp\n",
    "        \n",
    "        # Move to the next price\n",
    "        self.currentIndex += 1\n",
    "        \n",
    "        # Set Low boundary and Up boundary\n",
    "        #Set end state\n",
    "        if self.time <= 0 and self.state == 100: \n",
    "            done = True\n",
    "            reward = 20\n",
    "        elif self.time <= 0 and self.state >= 90:\n",
    "            done = True\n",
    "            reward = 10\n",
    "        elif self.time <= 0 and self.state >= 80:\n",
    "            done = True\n",
    "            reward = 5\n",
    "        elif self.time <= 0 and self.state < 80:\n",
    "            done = True\n",
    "            reward = -1000\n",
    "        elif self.state < 0 or self.state > 100:\n",
    "            done = True\n",
    "            reward = -1000\n",
    "        else:\n",
    "            done = False       \n",
    "\n",
    "        \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = np.array([20 + random.randint(-3,3)])#.astype(float)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        self.currentprice = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        self.open = 0\n",
    "        # Set total time \n",
    "        self.time = 23 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7a020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuche\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env=ChargeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5b3060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.63738], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1eb853d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f5d517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ed4e94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by the `reset()` method does not match the given observation space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37744\\1913415460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;31m# ============ Check the returned values ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m     \u001b[0m_check_returned_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error while checking key={key}: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0m_check_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"reset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;31m# Sample a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_obs\u001b[1;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"The observation returned by `{method_name}()` method must be a numpy array\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     assert observation_space.contains(\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     ), f\"The observation returned by the `{method_name}()` method does not match the given observation space\"\n",
      "\u001b[1;31mAssertionError\u001b[0m: The observation returned by the `reset()` method does not match the given observation space"
     ]
    }
   ],
   "source": [
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf520453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-993.869998395443 State:-20\n",
      "Episode:2 Score:-961.9999980330467 State:[40]\n",
      "Episode:3 Score:-962.7599990367889 State:120\n",
      "Episode:4 Score:-1013.2700015902519 State:-20\n",
      "Episode:5 Score:-1001.1399984359741 State:-20\n",
      "Episode:6 Score:46.2499982714653 State:[80]\n",
      "Episode:7 Score:-1048.3900007605553 State:120\n",
      "Episode:8 Score:-907.1150005459785 State:40\n",
      "Episode:9 Score:-999.9400015473366 State:-20\n",
      "Episode:10 Score:-1011.5500022768974 State:-20\n",
      "Episode:11 Score:-963.7799987792969 State:[-20]\n",
      "Episode:12 Score:-963.7799987792969 State:-20\n",
      "Episode:13 Score:-985.0699982047081 State:-20\n",
      "Episode:14 Score:-1008.320000231266 State:120\n",
      "Episode:15 Score:-1021.5500002503395 State:120\n",
      "Episode:16 Score:-1005.659999191761 State:40\n",
      "Episode:17 Score:-1041.8950009942055 State:[40]\n",
      "Episode:18 Score:-935.7999992370605 State:-20\n",
      "Episode:19 Score:-1037.7899996638298 State:[-20]\n",
      "Episode:20 Score:-1072.470000743866 State:120\n",
      "Episode:21 Score:-1012.210001707077 State:[120]\n",
      "Episode:22 Score:-1016.2349977493286 State:60\n",
      "Episode:23 Score:93.37000185251236 State:120\n",
      "Episode:24 Score:-1021.6599977016449 State:120\n",
      "Episode:25 Score:-1048.60999917984 State:120\n",
      "Episode:26 Score:35.01000088453293 State:80\n",
      "Episode:27 Score:-1024.050001680851 State:-20\n",
      "Episode:28 Score:-963.7799987792969 State:-20\n",
      "Episode:29 Score:-976.1949970126152 State:-20\n",
      "Episode:30 Score:-994.4849986433983 State:-20\n",
      "Episode:31 Score:-1022.2200016975403 State:-20\n",
      "Episode:32 Score:-935.7999992370605 State:-20\n",
      "Episode:33 Score:-1007.7199995517731 State:120\n",
      "Episode:34 Score:35.49000281095505 State:[80]\n",
      "Episode:35 Score:-935.7999992370605 State:-20\n",
      "Episode:36 Score:-969.8699966073036 State:-20\n",
      "Episode:37 Score:-1028.9300013184547 State:120\n",
      "Episode:38 Score:-1004.5999999046326 State:-20\n",
      "Episode:39 Score:91.59500259160995 State:80\n",
      "Episode:40 Score:-959.6099988818169 State:[-20]\n",
      "Episode:41 Score:116.53000098466873 State:100\n",
      "Episode:42 Score:-989.6400001645088 State:40\n",
      "Episode:43 Score:-1005.9549987316132 State:[120]\n",
      "Episode:44 Score:-1005.7399983406067 State:-20\n",
      "Episode:45 Score:-972.8399982452393 State:-20\n",
      "Episode:46 Score:-1014.2399981617928 State:-20\n",
      "Episode:47 Score:-963.7799987792969 State:-20\n",
      "Episode:48 Score:-979.5000021457672 State:120\n",
      "Episode:49 Score:-1014.4849995970726 State:-20\n",
      "Episode:50 Score:-969.6099995970726 State:-20\n"
     ]
    }
   ],
   "source": [
    "episodes = 50\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f6ce170",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac23d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, learning_rate=0.0001, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc68a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_27\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.7     |\n",
      "|    ep_rew_mean     | -915     |\n",
      "| time/              |          |\n",
      "|    fps             | 302      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 15           |\n",
      "|    ep_rew_mean          | -895         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 326          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093871355 |\n",
      "|    clip_fraction        | 0.0731       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.000546     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.05e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0108      |\n",
      "|    value_loss           | 4e+05        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37744\\42539359.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    308\u001b[0m     ) -> \"PPO\":\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         return super().learn(\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[1;31m# Normalize advantage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36mevaluate_actions\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[0mlatent_pi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[1;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[1;34m(self, action_logits)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"CategoricalDistribution\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m# Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1806d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72b3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7fe3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-49.61000108718872 State:[82]\n",
      "Episode:2 Score:-49.820000648498535 State:[82]\n",
      "Episode:3 Score:10.369999885559082 State:100\n",
      "Episode:4 Score:-8.189999103546143 State:100\n",
      "Episode:5 Score:-15.640000104904175 State:[100]\n",
      "Episode:6 Score:-214.33999919891357 State:[60]\n",
      "Episode:7 Score:-206.2300000190735 State:[79]\n",
      "Episode:8 Score:29.359999418258667 State:[98]\n",
      "Episode:9 Score:-262.35999870300293 State:[79]\n",
      "Episode:10 Score:49.555000483989716 State:100\n",
      "Episode:11 Score:-28.760001182556152 State:100\n",
      "Episode:12 Score:13.660000085830688 State:100\n",
      "Episode:13 Score:-9.789999723434448 State:[98]\n",
      "Episode:14 Score:-194.10000014305115 State:[57]\n",
      "Episode:15 Score:-200.0 State:[20]\n",
      "Episode:16 Score:-6.219999551773071 State:[81]\n",
      "Episode:17 Score:-227.660001039505 State:[78]\n",
      "Episode:18 Score:-23.199999809265137 State:[98]\n",
      "Episode:19 Score:21.305000066757202 State:100\n",
      "Episode:20 Score:3.720000207424164 State:[100]\n",
      "Episode:21 Score:-20.339999496936798 State:[99]\n",
      "Episode:22 Score:-1059.1700008511543 State:120\n",
      "Episode:23 Score:32.88999956846237 State:[99]\n",
      "Episode:24 Score:-12.224999189376831 State:[98]\n",
      "Episode:25 Score:-191.1299991607666 State:[78]\n",
      "Episode:26 Score:-8.699999630451202 State:[82]\n",
      "Episode:27 Score:20.89000105857849 State:[99]\n",
      "Episode:28 Score:32.36999988555908 State:[80]\n",
      "Episode:29 Score:53.56500023603439 State:[100]\n",
      "Episode:30 Score:-49.75000077486038 State:[99]\n",
      "Episode:31 Score:-229.3600013256073 State:[79]\n",
      "Episode:32 Score:-52.739999294281006 State:[99]\n",
      "Episode:33 Score:-243.6299991607666 State:[77]\n",
      "Episode:34 Score:-213.99999952316284 State:[77]\n",
      "Episode:35 Score:5.085000991821289 State:100\n",
      "Episode:36 Score:-241.4900000691414 State:[77]\n",
      "Episode:37 Score:-23.06499934196472 State:100\n",
      "Episode:38 Score:-17.820001125335693 State:[81]\n",
      "Episode:39 Score:-998.6800005435944 State:[120]\n",
      "Episode:40 Score:40.55500054359436 State:[99]\n",
      "Episode:41 Score:6.440000534057617 State:100\n",
      "Episode:42 Score:-37.41499900817871 State:[99]\n",
      "Episode:43 Score:-1057.8699996471405 State:120\n",
      "Episode:44 Score:-197.22000002861023 State:[37]\n",
      "Episode:45 Score:26.445000648498535 State:[81]\n",
      "Episode:46 Score:3.6499998569488525 State:[97]\n",
      "Episode:47 Score:4.255001068115234 State:100\n",
      "Episode:48 Score:19.015000581741333 State:100\n",
      "Episode:49 Score:3.525001049041748 State:[83]\n",
      "Episode:50 Score:41.59000110626221 State:[83]\n",
      "Episode:51 Score:-65.19000083208084 State:100\n",
      "Episode:52 Score:36.314999997615814 State:[98]\n",
      "Episode:53 Score:49.555000483989716 State:[100]\n",
      "Episode:54 Score:-4.889999628067017 State:[80]\n",
      "Episode:55 Score:-203.4949996471405 State:[78]\n",
      "Episode:56 Score:-178.48499965667725 State:[77]\n",
      "Episode:57 Score:-24.3600013256073 State:[80]\n",
      "Episode:58 Score:-206.27499961853027 State:[58]\n",
      "Episode:59 Score:-9.359999418258667 State:[81]\n",
      "Episode:60 Score:-39.160000801086426 State:100\n",
      "Episode:61 Score:-12.240001201629639 State:[97]\n",
      "Episode:62 Score:-2.229999542236328 State:[97]\n",
      "Episode:63 Score:-30.049999952316284 State:[98]\n",
      "Episode:64 Score:-31.09999918937683 State:100\n",
      "Episode:65 Score:16.055001258850098 State:[97]\n",
      "Episode:66 Score:-21.41000109910965 State:[81]\n",
      "Episode:67 Score:-27.539999961853027 State:[83]\n",
      "Episode:68 Score:34.30000019073486 State:[99]\n",
      "Episode:69 Score:-0.5600011348724365 State:100\n",
      "Episode:70 Score:10.310000121593475 State:100\n",
      "Episode:71 Score:-1049.4000008702278 State:120\n",
      "Episode:72 Score:-36.430000603199005 State:100\n",
      "Episode:73 Score:-1043.5899990200996 State:120\n",
      "Episode:74 Score:-22.73000168800354 State:[99]\n",
      "Episode:75 Score:-15.189999878406525 State:100\n",
      "Episode:76 Score:0.020000457763671875 State:[99]\n",
      "Episode:77 Score:3.455000877380371 State:[83]\n",
      "Episode:78 Score:-41.609999656677246 State:[99]\n",
      "Episode:79 Score:-3.5699995160102844 State:[97]\n",
      "Episode:80 Score:18.83000010251999 State:[80]\n",
      "Episode:81 Score:-5.725000381469727 State:[80]\n",
      "Episode:82 Score:-24.480000972747803 State:[97]\n",
      "Episode:83 Score:-32.3699996471405 State:[80]\n",
      "Episode:84 Score:-11.330000162124634 State:[82]\n",
      "Episode:85 Score:12.070000171661377 State:[98]\n",
      "Episode:86 Score:-54.600000858306885 State:[83]\n",
      "Episode:87 Score:-37.09999990463257 State:[83]\n",
      "Episode:88 Score:-31.460001230239868 State:100\n",
      "Episode:89 Score:-17.24000120162964 State:[83]\n",
      "Episode:90 Score:-4.819999992847443 State:[80]\n",
      "Episode:91 Score:13.250000238418579 State:100\n",
      "Episode:92 Score:7.510000467300415 State:[99]\n",
      "Episode:93 Score:8.734999358654022 State:100\n",
      "Episode:94 Score:-32.059998989105225 State:[82]\n",
      "Episode:95 Score:-30.399999856948853 State:[83]\n",
      "Episode:96 Score:-2.239999294281006 State:[81]\n",
      "Episode:97 Score:33.92000025510788 State:[99]\n",
      "Episode:98 Score:-216.61999940872192 State:[60]\n",
      "Episode:99 Score:-25.149999618530273 State:[97]\n",
      "Episode:100 Score:16.27999997138977 State:[100]\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(obs)\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f114198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('PPO_model', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View logs in Tensorboard\n",
    "training_log_path = os.path.join(log_path, 'PPO')\n",
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a61ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
