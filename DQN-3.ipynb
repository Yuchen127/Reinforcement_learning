{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e620552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "306060d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Peice array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start SOC\n",
    "        self.state = 20 + random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        self.currentprice = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        self.open = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 sell\n",
    "        # 1 -1 = 0 hold\n",
    "        # 2 -1 = 1 buy \n",
    "        self.state += (action - 1) * self.power  \n",
    "        self.currentprice = self.priceArray[self.currentIndex]\n",
    "\n",
    "        \n",
    "        \n",
    "        if 100 < self.state < (self.max_state + self.power):\n",
    "            self.state = 100\n",
    "        elif (0 - self.power) < self.state < 0:\n",
    "            self.state = 0\n",
    "        # Calculate reward    \n",
    "        # BUy\n",
    "        # Give plus reward when charge at low proce, and minus at high price, and 0 at hold\n",
    "        if self.currentprice < (self.mid - 2):\n",
    "            temp = abs(self.mid - self.currentprice) * (action - 1)\n",
    "            reward = temp\n",
    "            \n",
    "        # Sell\n",
    "        # Give minus reward when discharge at low proce, and plus at high price, and 0 at hold\n",
    "        elif self.currentprice > (self.mid + 2):\n",
    "            temp = -abs(self.currentprice - self.mid) * (action - 1)\n",
    "            reward = temp\n",
    "            \n",
    "        # Hold\n",
    "        # Give plus reward when hold in median price range, and 0 reward otherwise\n",
    "        else:\n",
    "            temp = abs(abs(action) - 1) * self.currentprice\n",
    "            reward = temp\n",
    "        \n",
    "        # Move to the next price\n",
    "        self.currentIndex += 1\n",
    "        \n",
    "        # Set Low boundary and Up boundary\n",
    "        # Set end state\n",
    "        if self.currentIndex == len(self.priceArray)-1 and self.state == 100:\n",
    "            done = True\n",
    "            reward += 50\n",
    "        elif self.currentIndex == len(self.priceArray)-1 and self.state >= 90:\n",
    "            done = True\n",
    "            reward += 30\n",
    "        elif self.currentIndex == len(self.priceArray)-1 and self.state >= 80:\n",
    "            done = True\n",
    "            reward += 20               \n",
    "        elif self.currentIndex == len(self.priceArray)-1 and self.state < 80:\n",
    "            done = True\n",
    "            reward += -100           \n",
    "        elif self.state < 0 or self.state > 100:\n",
    "            done = True\n",
    "            reward = -1000\n",
    "        else:\n",
    "            done = False       \n",
    "\n",
    "        \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 20 + random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        self.currentprice = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        self.open = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fbad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChargeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca77928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.072573], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "552762b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-996.3349981307983 State:120\n",
      "Episode:2 Score:-978.9249992370605 State:-20\n",
      "Episode:3 Score:-922.835000038147 State:120\n",
      "Episode:4 Score:-29.17000102996826 State:40\n",
      "Episode:5 Score:-1003.3150043487549 State:-20\n",
      "Episode:6 Score:-95.33999729156494 State:40\n",
      "Episode:7 Score:13.15000057220459 State:60\n",
      "Episode:8 Score:-974.3549995422363 State:120\n",
      "Episode:9 Score:-971.4549989700317 State:-20\n",
      "Episode:10 Score:-1000.6699981689453 State:120\n",
      "Episode:11 Score:-99.69999980926514 State:60\n",
      "Episode:12 Score:-978.9249992370605 State:-20\n",
      "Episode:13 Score:-970.2750015258789 State:-20\n",
      "Episode:14 Score:-26.38999652862549 State:81\n",
      "Episode:15 Score:-6.190000534057617 State:3\n",
      "Episode:16 Score:-96.5550012588501 State:40\n",
      "Episode:17 Score:-974.2849969863892 State:120\n",
      "Episode:18 Score:-990.3650007247925 State:-20\n",
      "Episode:19 Score:-123.82000064849854 State:79\n",
      "Episode:20 Score:-1019.3599996566772 State:120\n",
      "Episode:21 Score:-1002.7450017929077 State:-20\n",
      "Episode:22 Score:-947.5499992370605 State:-20\n",
      "Episode:23 Score:-60.03499984741211 State:60\n",
      "Episode:24 Score:-13.579998016357422 State:-20\n",
      "Episode:25 Score:29.375001907348633 State:97\n",
      "Episode:26 Score:-978.9249992370605 State:-20\n",
      "Episode:27 Score:-987.1650009155273 State:-20\n",
      "Episode:28 Score:-961.3100004196167 State:-20\n",
      "Episode:29 Score:-1018.2449998855591 State:-20\n",
      "Episode:30 Score:-1001.6199989318848 State:-20\n",
      "Episode:31 Score:-1027.6050004959106 State:-20\n",
      "Episode:32 Score:-1017.5550003051758 State:-20\n",
      "Episode:33 Score:-971.1200008392334 State:-20\n",
      "Episode:34 Score:-1002.3049983978271 State:-20\n",
      "Episode:35 Score:57.704999923706055 State:81\n",
      "Episode:36 Score:33.91000175476074 State:120\n",
      "Episode:37 Score:-999.5100030899048 State:-20\n",
      "Episode:38 Score:53.24000072479248 State:83\n",
      "Episode:39 Score:-1018.7850017547607 State:-20\n",
      "Episode:40 Score:-968.995002746582 State:-20\n",
      "Episode:41 Score:-957.7250003814697 State:120\n",
      "Episode:42 Score:-976.3300008773804 State:120\n",
      "Episode:43 Score:-76.22000026702881 State:1\n",
      "Episode:44 Score:-978.9249992370605 State:-20\n",
      "Episode:45 Score:-995.9149980545044 State:120\n",
      "Episode:46 Score:-977.3800010681152 State:120\n",
      "Episode:47 Score:-978.9249992370605 State:-20\n",
      "Episode:48 Score:4.815003395080566 State:80\n",
      "Episode:49 Score:30.265002250671387 State:80\n",
      "Episode:50 Score:-982.164999961853 State:-20\n",
      "Episode:51 Score:87.96500492095947 State:80\n",
      "Episode:52 Score:5.080002784729004 State:80\n",
      "Episode:53 Score:-987.1650009155273 State:-20\n",
      "Episode:54 Score:20.519997596740723 State:43\n",
      "Episode:55 Score:49.13500118255615 State:120\n",
      "Episode:56 Score:-978.9249992370605 State:-20\n",
      "Episode:57 Score:-961.0349988937378 State:-20\n",
      "Episode:58 Score:-985.9349966049194 State:120\n",
      "Episode:59 Score:-989.4699993133545 State:-20\n",
      "Episode:60 Score:-78.36999607086182 State:40\n",
      "Episode:61 Score:-54.8849983215332 State:40\n",
      "Episode:62 Score:-989.4699993133545 State:-20\n",
      "Episode:63 Score:-978.9249992370605 State:-20\n",
      "Episode:64 Score:-978.9249992370605 State:-20\n",
      "Episode:65 Score:-987.1650009155273 State:-20\n",
      "Episode:66 Score:-1028.4749994277954 State:120\n",
      "Episode:67 Score:-43.105000495910645 State:0\n",
      "Episode:68 Score:-83.27999973297119 State:60\n",
      "Episode:69 Score:-987.1650009155273 State:-20\n",
      "Episode:70 Score:-1017.5999994277954 State:120\n",
      "Episode:71 Score:-987.1650009155273 State:-20\n",
      "Episode:72 Score:-1032.1449995040894 State:-20\n",
      "Episode:73 Score:117.90999984741211 State:100\n",
      "Episode:74 Score:-47.239996910095215 State:19\n",
      "Episode:75 Score:-1022.7600021362305 State:-20\n",
      "Episode:76 Score:-978.9249992370605 State:-20\n",
      "Episode:77 Score:-974.350001335144 State:-20\n",
      "Episode:78 Score:-23.84500503540039 State:-20\n",
      "Episode:79 Score:10.735003471374512 State:80\n",
      "Episode:80 Score:-999.7449998855591 State:-20\n",
      "Episode:81 Score:-24.219996452331543 State:21\n",
      "Episode:82 Score:-1038.954999923706 State:-20\n",
      "Episode:83 Score:-978.9249992370605 State:-20\n",
      "Episode:84 Score:-1020.979998588562 State:120\n",
      "Episode:85 Score:-997.524998664856 State:-20\n",
      "Episode:86 Score:-1052.9600019454956 State:-20\n",
      "Episode:87 Score:-38.915000915527344 State:21\n",
      "Episode:88 Score:-982.0699996948242 State:120\n",
      "Episode:89 Score:-997.524998664856 State:-20\n",
      "Episode:90 Score:-996.479998588562 State:-20\n",
      "Episode:91 Score:-978.9249992370605 State:-20\n",
      "Episode:92 Score:-1022.5749998092651 State:-20\n",
      "Episode:93 Score:-1010.229998588562 State:120\n",
      "Episode:94 Score:-1018.7850017547607 State:-20\n",
      "Episode:95 Score:-991.2199974060059 State:-20\n",
      "Episode:96 Score:-978.9249992370605 State:-20\n",
      "Episode:97 Score:-96.75499820709229 State:60\n",
      "Episode:98 Score:-1034.3600025177002 State:-20\n",
      "Episode:99 Score:-961.6949996948242 State:120\n",
      "Episode:100 Score:-92.37500286102295 State:0\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4bc362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4592ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2eabdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c197b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply when 'Sequential' object has no attribute '_compile_time_distribution_strategy'\n",
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ce45bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "558918c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 24)                48        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "feafaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dfd500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 128s 13ms/step - reward: -40.7130\n",
      "694 episodes - episode_reward: -586.676 [-1024.010, 33.625] - loss: 14886.497 - mse: 10783.019 - mean_q: -10.155\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: -3.6476\n",
      "437 episodes - episode_reward: -83.494 [-978.925, -6.025] - loss: 12120.757 - mse: 10030.620 - mean_q: -19.426\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: -3.4303\n",
      "435 episodes - episode_reward: -78.756 [-128.875, -5.740] - loss: 5982.479 - mse: 8963.064 - mean_q: -46.978\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: -3.5568\n",
      "435 episodes - episode_reward: -81.835 [-121.070, -17.840] - loss: 3013.865 - mse: 16049.631 - mean_q: -64.845\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: -2.3317\n",
      "done, took 622.405 seconds\n"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mse']) #mean squared error mse, mean absloute error mae.\n",
    "\n",
    "#my_callbacks = [\n",
    "    #EarlyStopping(patience=2),\n",
    "    #ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #CSVLogger('training.log')]\n",
    "\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19df7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -40.070, steps: 23\n",
      "Episode 2: reward: -40.070, steps: 23\n",
      "Episode 3: reward: -40.070, steps: 23\n",
      "Episode 4: reward: -40.070, steps: 23\n",
      "Episode 5: reward: -40.070, steps: 23\n",
      "Episode 6: reward: -40.070, steps: 23\n",
      "Episode 7: reward: -40.070, steps: 23\n",
      "Episode 8: reward: -40.070, steps: 23\n",
      "Episode 9: reward: -40.070, steps: 23\n",
      "Episode 10: reward: -40.070, steps: 23\n",
      "Episode 11: reward: -40.070, steps: 23\n",
      "Episode 12: reward: -40.070, steps: 23\n",
      "Episode 13: reward: -40.070, steps: 23\n",
      "Episode 14: reward: -40.070, steps: 23\n",
      "Episode 15: reward: -40.070, steps: 23\n",
      "Episode 16: reward: -40.070, steps: 23\n",
      "Episode 17: reward: -40.070, steps: 23\n",
      "Episode 18: reward: -40.070, steps: 23\n",
      "Episode 19: reward: -40.070, steps: 23\n",
      "Episode 20: reward: -40.070, steps: 23\n",
      "Episode 21: reward: -40.070, steps: 23\n",
      "Episode 22: reward: -40.070, steps: 23\n",
      "Episode 23: reward: -40.070, steps: 23\n",
      "Episode 24: reward: -40.070, steps: 23\n",
      "Episode 25: reward: -40.070, steps: 23\n",
      "Episode 26: reward: -40.070, steps: 23\n",
      "Episode 27: reward: -40.070, steps: 23\n",
      "Episode 28: reward: -40.070, steps: 23\n",
      "Episode 29: reward: -40.070, steps: 23\n",
      "Episode 30: reward: -40.070, steps: 23\n",
      "Episode 31: reward: -40.070, steps: 23\n",
      "Episode 32: reward: -40.070, steps: 23\n",
      "Episode 33: reward: -40.070, steps: 23\n",
      "Episode 34: reward: -40.070, steps: 23\n",
      "Episode 35: reward: -40.070, steps: 23\n",
      "Episode 36: reward: -40.070, steps: 23\n",
      "Episode 37: reward: -40.070, steps: 23\n",
      "Episode 38: reward: -40.070, steps: 23\n",
      "Episode 39: reward: -40.070, steps: 23\n",
      "Episode 40: reward: -40.070, steps: 23\n",
      "Episode 41: reward: -40.070, steps: 23\n",
      "Episode 42: reward: -40.070, steps: 23\n",
      "Episode 43: reward: -40.070, steps: 23\n",
      "Episode 44: reward: -40.070, steps: 23\n",
      "Episode 45: reward: -40.070, steps: 23\n",
      "Episode 46: reward: -40.070, steps: 23\n",
      "Episode 47: reward: -40.070, steps: 23\n",
      "Episode 48: reward: -40.070, steps: 23\n",
      "Episode 49: reward: -40.070, steps: 23\n",
      "Episode 50: reward: -40.070, steps: 23\n",
      "Episode 51: reward: -40.070, steps: 23\n",
      "Episode 52: reward: -40.070, steps: 23\n",
      "Episode 53: reward: -40.070, steps: 23\n",
      "Episode 54: reward: -40.070, steps: 23\n",
      "Episode 55: reward: -40.070, steps: 23\n",
      "Episode 56: reward: -40.070, steps: 23\n",
      "Episode 57: reward: -40.070, steps: 23\n",
      "Episode 58: reward: -40.070, steps: 23\n",
      "Episode 59: reward: -40.070, steps: 23\n",
      "Episode 60: reward: -40.070, steps: 23\n",
      "Episode 61: reward: -40.070, steps: 23\n",
      "Episode 62: reward: -40.070, steps: 23\n",
      "Episode 63: reward: -40.070, steps: 23\n",
      "Episode 64: reward: -40.070, steps: 23\n",
      "Episode 65: reward: -40.070, steps: 23\n",
      "Episode 66: reward: -40.070, steps: 23\n",
      "Episode 67: reward: -40.070, steps: 23\n",
      "Episode 68: reward: -40.070, steps: 23\n",
      "Episode 69: reward: -40.070, steps: 23\n",
      "Episode 70: reward: -40.070, steps: 23\n",
      "Episode 71: reward: -40.070, steps: 23\n",
      "Episode 72: reward: -40.070, steps: 23\n",
      "Episode 73: reward: -40.070, steps: 23\n",
      "Episode 74: reward: -40.070, steps: 23\n",
      "Episode 75: reward: -40.070, steps: 23\n",
      "Episode 76: reward: -40.070, steps: 23\n",
      "Episode 77: reward: -40.070, steps: 23\n",
      "Episode 78: reward: -40.070, steps: 23\n",
      "Episode 79: reward: -40.070, steps: 23\n",
      "Episode 80: reward: -40.070, steps: 23\n",
      "Episode 81: reward: -40.070, steps: 23\n",
      "Episode 82: reward: -40.070, steps: 23\n",
      "Episode 83: reward: -40.070, steps: 23\n",
      "Episode 84: reward: -40.070, steps: 23\n",
      "Episode 85: reward: -40.070, steps: 23\n",
      "Episode 86: reward: -40.070, steps: 23\n",
      "Episode 87: reward: -40.070, steps: 23\n",
      "Episode 88: reward: -40.070, steps: 23\n",
      "Episode 89: reward: -40.070, steps: 23\n",
      "Episode 90: reward: -40.070, steps: 23\n",
      "Episode 91: reward: -40.070, steps: 23\n",
      "Episode 92: reward: -40.070, steps: 23\n",
      "Episode 93: reward: -40.070, steps: 23\n",
      "Episode 94: reward: -40.070, steps: 23\n",
      "Episode 95: reward: -40.070, steps: 23\n",
      "Episode 96: reward: -40.070, steps: 23\n",
      "Episode 97: reward: -40.070, steps: 23\n",
      "Episode 98: reward: -40.070, steps: 23\n",
      "Episode 99: reward: -40.070, steps: 23\n",
      "Episode 100: reward: -40.070, steps: 23\n",
      "-40.0699987411499\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f826195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
