{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9e620552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "306060d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Observation box\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start SOC\n",
    "        self.state = 20 + random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        self.rate = 0\n",
    "        self.profit = 0\n",
    "        self.per = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 - buy\n",
    "        # 1 - sell\n",
    "        # 2 - hold \n",
    " \n",
    "        \n",
    "        \n",
    "        # If state exceeds boundaries after the action due to power limit, set state to 100 or 0\n",
    "        # change action based on new state\n",
    "\n",
    "        # Hold when state >= 90 while approaching the end\n",
    "        if self.state >= 90 and (len(self.priceArray) - 1) - self.currentIndex < 3:\n",
    "            action = 2\n",
    "        # Buy when state < 100 while approaching the end\n",
    "        elif (self.max_state - self.state - 20) / self.power >= (len(self.priceArray) - 1) - self.currentIndex:\n",
    "            action = 0\n",
    "            \n",
    "        # Calculate reward    \n",
    "        # BUy\n",
    "        # Give - reward\n",
    "        if action == 0:\n",
    "            self.change = 1\n",
    "            if (self.max_state - self.state) < self.power:\n",
    "                self.per = self.max_state - self.state\n",
    "                self.rate = self.per / self.power\n",
    "                self.state = 80\n",
    "            else:\n",
    "                self.rate = 1\n",
    "                \n",
    "            if self.priceArray[self.currentIndex] < self.mid - 2 and self.profit >=0 :\n",
    "                reward = self.priceArray[self.currentIndex] * 5 #* ((self.max_state - self.per) / self.max_state) \n",
    "            else:\n",
    "                reward = -1 * self.priceArray[self.currentIndex]\n",
    "                \n",
    "            self.temp = -self.priceArray[self.currentIndex] * self.rate \n",
    "                    \n",
    "        # Sell\n",
    "        # Give + reward\n",
    "        elif action == 1:\n",
    "            self.change = -1\n",
    "            if self.state < self.power :\n",
    "                self.per = self.state\n",
    "                self.rate = self.state / self.power\n",
    "                self.state = 20\n",
    "            else:\n",
    "                self.rate = 1\n",
    "                \n",
    "            if self.priceArray[self.currentIndex] > self.mid + 2 and self.profit >=0 :\n",
    "                reward = 1 * self.priceArray[self.currentIndex] * 5 #*(self.per / self.max_state) \n",
    "            else:\n",
    "                reward = -1 * self.priceArray[self.currentIndex]\n",
    "            self.temp = self.priceArray[self.currentIndex] * self.rate \n",
    "                \n",
    "        # Hold        \n",
    "        else:\n",
    "            self.change = 0\n",
    "            if abs(self.priceArray[self.currentIndex] - self.mid) >= 2:\n",
    "                reward = self.priceArray[self.currentIndex]\n",
    "            else:\n",
    "                reward = -1 * self.priceArray[self.currentIndex]\n",
    "            self.temp = 0\n",
    "        \n",
    "        self.profit += self.temp\n",
    "        #reward += self.profit\n",
    "      \n",
    "            \n",
    "        self.state += self.change * self.power\n",
    "        self.per = self.state\n",
    "            \n",
    "        # Move to the next price\n",
    "        self.currentIndex += 1\n",
    "        \n",
    "        # Set Low boundary and Up boundary\n",
    "        #Set end state\n",
    "        if self.currentIndex == len(self.priceArray)-1: \n",
    "            done = True\n",
    "            if self.state < 80:\n",
    "                reward = -10000\n",
    "            if self.profit < 0:\n",
    "                reward = -1000\n",
    "        elif self.state < -0 or self.state > 100:\n",
    "                done = True\n",
    "                reward = -1000\n",
    "        elif self.state >= 80 and self.profit > 0:\n",
    "                done = True\n",
    "                reward = 1000\n",
    "        else:\n",
    "            done = False       \n",
    "\n",
    "        \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info ,self.profit\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 20 + random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        self.rate = 0\n",
    "        self.profit = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0fbad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChargeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ca77928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.73381], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "552762b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-977.4400048851967 State:80 p:-74.15000180006027\n",
      "Episode:2 Score:-1020.480000436306 State:60 p:-62.95049940347671\n",
      "Episode:3 Score:-619.4399985671043 State:100 p:-32.71099944114685\n",
      "Episode:4 Score:-1180.860003054142 State:80 p:-129.02700178027152\n",
      "Episode:5 Score:-579.1399993300438 State:80 p:-70.76099820137024\n",
      "Episode:6 Score:-1165.4400028586388 State:80 p:-136.42000246047974\n",
      "Episode:7 Score:-754.8200015425682 State:60 p:-15.95000247955322\n",
      "Episode:8 Score:-1170.180002629757 State:80 p:-106.21799936294556\n",
      "Episode:9 Score:-1186.9200006127357 State:100 p:-52.56000143289566\n",
      "Episode:10 Score:-884.7399994730949 State:80 p:-102.96800132989884\n",
      "Episode:11 Score:-1010.3399997353554 State:80 p:-61.94700312614441\n",
      "Episode:12 Score:-1185.5000029206276 State:100 p:-97.89900096654893\n",
      "Episode:13 Score:-659.3800048232079 State:80 p:-64.91000058650971\n",
      "Episode:14 Score:-543.3799949288368 State:63 p:-25.389998018741608\n",
      "Episode:15 Score:-1042.9200039505959 State:80 p:-78.86999970674515\n",
      "Episode:16 Score:-1184.9999999403954 State:100 p:-87.52000212669373\n",
      "Episode:17 Score:-621.7799997925758 State:80 p:-15.98599947690964\n",
      "Episode:18 Score:-846.9200053811073 State:80 p:-69.54000288248062\n",
      "Episode:19 Score:-1102.939999639988 State:100 p:-90.3665009379387\n",
      "Episode:20 Score:-818.2999945282936 State:80 p:-51.651997888088225\n",
      "Episode:21 Score:-1002.3399987816811 State:100 p:-73.7999993443489\n",
      "Episode:22 Score:-1182.3400011658669 State:100 p:-130.62349985837938\n",
      "Episode:23 Score:-1062.8799986243248 State:80 p:-38.46999809741974\n",
      "Episode:24 Score:-1033.859999716282 State:60 p:-77.13000029325485\n",
      "Episode:25 Score:-10010.459995806217 State:60 p:6.297003388404846\n",
      "Episode:26 Score:-771.3400020003319 State:80 p:-64.76800189018249\n",
      "Episode:27 Score:-947.5799999833107 State:100 p:-41.859998881816864\n",
      "Episode:28 Score:-861.379995405674 State:80 p:-76.3699987411499\n",
      "Episode:29 Score:-701.9199950098991 State:80 p:-36.61099886894226\n",
      "Episode:30 Score:-851.1200047135353 State:60 p:-53.44999868869782\n",
      "Episode:31 Score:-780.2399949431419 State:100 p:-52.212999200820924\n",
      "Episode:32 Score:-1185.5400032401085 State:80 p:-90.82100104093551\n",
      "Episode:33 Score:-1123.40000385046 State:80 p:-108.23950017690659\n",
      "Episode:34 Score:-9744.359993159771 State:43 p:46.6100007891655\n",
      "Episode:35 Score:-1154.0800023674965 State:80 p:-128.84000009298325\n",
      "Episode:36 Score:-1078.4800000786781 State:60 p:-12.825996649265292\n",
      "Episode:37 Score:-1161.2200003266335 State:100 p:-99.07150090932846\n",
      "Episode:38 Score:-693.7799970507622 State:80 p:-66.95299922227859\n",
      "Episode:39 Score:-1110.340002477169 State:80 p:-127.30600135326385\n",
      "Episode:40 Score:-981.1200004220009 State:80 p:-61.022999787330626\n",
      "Episode:41 Score:-1211.280001103878 State:80 p:-97.98000237941741\n",
      "Episode:42 Score:-1078.580003798008 State:80 p:-118.58699981570244\n",
      "Episode:43 Score:-561.4599985480309 State:80 p:-45.85999917984009\n",
      "Episode:44 Score:-1031.439999639988 State:100 p:-58.25899959802628\n",
      "Episode:45 Score:-1146.120001733303 State:80 p:-58.73450264930725\n",
      "Episode:46 Score:-1200.9800019860268 State:60 p:-105.10900062322617\n",
      "Episode:47 Score:-441.64000099897385 State:60 p:-17.00999814271927\n",
      "Episode:48 Score:-464.0199981331825 State:80 p:-29.089997947216034\n",
      "Episode:49 Score:-1099.979999601841 State:100 p:-113.80849956274032\n",
      "Episode:50 Score:-627.3600010275841 State:80 p:-44.33999974727631\n",
      "Episode:51 Score:-573.3199973702431 State:80 p:-74.05299935340881\n",
      "Episode:52 Score:1258.5899992585182 State:80 p:2.4409999370574944\n",
      "Episode:53 Score:-857.4199977517128 State:100 p:-29.65949989557266\n",
      "Episode:54 Score:-758.0400047898293 State:80 p:-80.8180016040802\n",
      "Episode:55 Score:1387.3400037884712 State:80 p:17.7990017414093\n",
      "Episode:56 Score:-937.4800013899803 State:60 p:-47.4489987373352\n",
      "Episode:57 Score:-1148.1600027680397 State:80 p:-136.11149920225142\n",
      "Episode:58 Score:1321.700004041195 State:80 p:2.5900001049041776\n",
      "Episode:59 Score:-1050.1200013756752 State:100 p:-61.22800089120865\n",
      "Episode:60 Score:-679.9800083041191 State:80 p:-39.458001530170435\n",
      "Episode:61 Score:-1089.840001642704 State:100 p:-88.44999974966049\n",
      "Episode:62 Score:-669.7599961161613 State:80 p:-67.34099913835526\n",
      "Episode:63 Score:-969.3599959015846 State:80 p:-119.09199709892273\n",
      "Episode:64 Score:1177.590006172657 State:83 p:3.2900022864341736\n",
      "Episode:65 Score:-1270.120001256466 State:80 p:-93.95150049924851\n",
      "Episode:66 Score:-1204.0800014138222 State:60 p:-72.30199919939041\n",
      "Episode:67 Score:-1061.5600038170815 State:80 p:-110.10050092935562\n",
      "Episode:68 Score:-749.7999946475029 State:80 p:-3.9009978890419035\n",
      "Episode:69 Score:-1212.180002272129 State:80 p:-54.89449926912785\n",
      "Episode:70 Score:-1101.119998037815 State:63 p:-47.840000331401825\n",
      "Episode:71 Score:-1023.020002424717 State:80 p:-73.53999823331833\n",
      "Episode:72 Score:-1273.7800012230873 State:100 p:-167.32000237703323\n",
      "Episode:73 Score:-1244.0800027251244 State:100 p:-85.55300025939941\n",
      "Episode:74 Score:-784.8799981474876 State:80 p:-61.58099765777588\n",
      "Episode:75 Score:-780.3200020194054 State:100 p:-33.77899980545044\n",
      "Episode:76 Score:-1218.019999563694 State:100 p:-82.11350125074387\n",
      "Episode:77 Score:-705.0600044131279 State:80 p:-62.299001562595365\n",
      "Episode:78 Score:-1080.04000467062 State:60 p:-60.25950117111206\n",
      "Episode:79 Score:-472.06000179052353 State:80 p:-13.482999372482297\n",
      "Episode:80 Score:-670.1000009179115 State:100 p:-51.512998378276826\n",
      "Episode:81 Score:-1117.0799999833107 State:100 p:-97.1200003027916\n",
      "Episode:82 Score:-1058.4599962830544 State:80 p:-42.646999788284305\n",
      "Episode:83 Score:-678.6599989533424 State:80 p:-71.02299765348434\n",
      "Episode:84 Score:-1191.1000036597252 State:80 p:-103.9790002822876\n",
      "Episode:85 Score:-730.9600047469139 State:80 p:-51.00000047683716\n",
      "Episode:86 Score:-1075.9200062155724 State:80 p:-122.25000166893005\n",
      "Episode:87 Score:-1261.7800006270409 State:60 p:-32.505999732017514\n",
      "Episode:88 Score:-1176.9600034356117 State:80 p:-90.51700187921524\n",
      "Episode:89 Score:-661.859999358654 State:60 p:-11.122998631000513\n",
      "Episode:90 Score:-1140.1800041794777 State:77 p:-105.15000039339066\n",
      "Episode:91 Score:1034.2500016093254 State:81 p:2.2000009417533875\n",
      "Episode:92 Score:-1202.3800011277199 State:80 p:-85.51200122833252\n",
      "Episode:93 Score:-204.6200013756752 State:80 p:-26.990999650955203\n",
      "Episode:94 Score:-1039.8000054955482 State:60 p:-69.92550018727779\n",
      "Episode:95 Score:-1221.7799993157387 State:80 p:-117.31949714422225\n",
      "Episode:96 Score:-879.9199914336205 State:100 p:-49.44599871635437\n",
      "Episode:97 Score:1491.099993288517 State:80 p:7.100998830795291\n",
      "Episode:98 Score:1168.030003964901 State:80 p:2.4270016789436326\n",
      "Episode:99 Score:-893.1800003647804 State:62 p:-22.759999692440033\n",
      "Episode:100 Score:1176.2500064969063 State:80 p:2.3900017142295837\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info, p = env.step(action) \n",
    "        score+=reward\n",
    "    #print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))\n",
    "    print('Episode:{} Score:{} State:{} p:{}'.format(episode, score, n_state, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e4bc362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4592ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d2eabdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear')) #linear, softmax\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c197b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply when 'Sequential' object has no attribute '_compile_time_distribution_strategy'\n",
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2ce45bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "558918c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 24)                48        \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "feafaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dfd500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 40000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 97s 10ms/step - reward: -31.9161\n",
      "435 episodes - episode_reward: -735.230 [-10052.250, 1401.600] - loss: 29416.277 - mse: 50414.204 - mean_q: -102.666\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -18.6077\n",
      "435 episodes - episode_reward: -427.843 [-660.740, -348.360] - loss: 20839.211 - mse: 98825.258 - mean_q: -174.019\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -18.2924\n",
      "435 episodes - episode_reward: -419.600 [-419.600, -419.600] - loss: 17412.863 - mse: 92638.172 - mean_q: -150.864\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -18.2506\n",
      "done, took 394.041 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2216eaed6a0>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mse']) #mean squared error mse, mean absloute error mae.\n",
    "dqn.fit(env, nb_steps=40000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "19df7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -419.600, steps: 23\n",
      "Episode 2: reward: -419.600, steps: 23\n",
      "Episode 3: reward: -419.600, steps: 23\n",
      "Episode 4: reward: -419.600, steps: 23\n",
      "Episode 5: reward: -419.600, steps: 23\n",
      "Episode 6: reward: -419.600, steps: 23\n",
      "Episode 7: reward: -419.600, steps: 23\n",
      "Episode 8: reward: -419.600, steps: 23\n",
      "Episode 9: reward: -419.600, steps: 23\n",
      "Episode 10: reward: -419.600, steps: 23\n",
      "Episode 11: reward: -419.600, steps: 23\n",
      "Episode 12: reward: -419.600, steps: 23\n",
      "Episode 13: reward: -419.600, steps: 23\n",
      "Episode 14: reward: -419.600, steps: 23\n",
      "Episode 15: reward: -419.600, steps: 23\n",
      "Episode 16: reward: -419.600, steps: 23\n",
      "Episode 17: reward: -419.600, steps: 23\n",
      "Episode 18: reward: -419.600, steps: 23\n",
      "Episode 19: reward: -419.600, steps: 23\n",
      "Episode 20: reward: -419.600, steps: 23\n",
      "Episode 21: reward: -419.600, steps: 23\n",
      "Episode 22: reward: -419.600, steps: 23\n",
      "Episode 23: reward: -419.600, steps: 23\n",
      "Episode 24: reward: -419.600, steps: 23\n",
      "Episode 25: reward: -419.600, steps: 23\n",
      "Episode 26: reward: -419.600, steps: 23\n",
      "Episode 27: reward: -419.600, steps: 23\n",
      "Episode 28: reward: -419.600, steps: 23\n",
      "Episode 29: reward: -419.600, steps: 23\n",
      "Episode 30: reward: -419.600, steps: 23\n",
      "Episode 31: reward: -419.600, steps: 23\n",
      "Episode 32: reward: -419.600, steps: 23\n",
      "Episode 33: reward: -419.600, steps: 23\n",
      "Episode 34: reward: -419.600, steps: 23\n",
      "Episode 35: reward: -419.600, steps: 23\n",
      "Episode 36: reward: -419.600, steps: 23\n",
      "Episode 37: reward: -419.600, steps: 23\n",
      "Episode 38: reward: -419.600, steps: 23\n",
      "Episode 39: reward: -419.600, steps: 23\n",
      "Episode 40: reward: -419.600, steps: 23\n",
      "Episode 41: reward: -419.600, steps: 23\n",
      "Episode 42: reward: -419.600, steps: 23\n",
      "Episode 43: reward: -419.600, steps: 23\n",
      "Episode 44: reward: -419.600, steps: 23\n",
      "Episode 45: reward: -419.600, steps: 23\n",
      "Episode 46: reward: -419.600, steps: 23\n",
      "Episode 47: reward: -419.600, steps: 23\n",
      "Episode 48: reward: -419.600, steps: 23\n",
      "Episode 49: reward: -419.600, steps: 23\n",
      "Episode 50: reward: -419.600, steps: 23\n",
      "Episode 51: reward: -419.600, steps: 23\n",
      "Episode 52: reward: -419.600, steps: 23\n",
      "Episode 53: reward: -419.600, steps: 23\n",
      "Episode 54: reward: -419.600, steps: 23\n",
      "Episode 55: reward: -419.600, steps: 23\n",
      "Episode 56: reward: -419.600, steps: 23\n",
      "Episode 57: reward: -419.600, steps: 23\n",
      "Episode 58: reward: -419.600, steps: 23\n",
      "Episode 59: reward: -419.600, steps: 23\n",
      "Episode 60: reward: -419.600, steps: 23\n",
      "Episode 61: reward: -419.600, steps: 23\n",
      "Episode 62: reward: -419.600, steps: 23\n",
      "Episode 63: reward: -419.600, steps: 23\n",
      "Episode 64: reward: -419.600, steps: 23\n",
      "Episode 65: reward: -419.600, steps: 23\n",
      "Episode 66: reward: -419.600, steps: 23\n",
      "Episode 67: reward: -419.600, steps: 23\n",
      "Episode 68: reward: -419.600, steps: 23\n",
      "Episode 69: reward: -419.600, steps: 23\n",
      "Episode 70: reward: -419.600, steps: 23\n",
      "Episode 71: reward: -419.600, steps: 23\n",
      "Episode 72: reward: -419.600, steps: 23\n",
      "Episode 73: reward: -419.600, steps: 23\n",
      "Episode 74: reward: -419.600, steps: 23\n",
      "Episode 75: reward: -419.600, steps: 23\n",
      "Episode 76: reward: -419.600, steps: 23\n",
      "Episode 77: reward: -419.600, steps: 23\n",
      "Episode 78: reward: -419.600, steps: 23\n",
      "Episode 79: reward: -419.600, steps: 23\n",
      "Episode 80: reward: -419.600, steps: 23\n",
      "Episode 81: reward: -419.600, steps: 23\n",
      "Episode 82: reward: -419.600, steps: 23\n",
      "Episode 83: reward: -419.600, steps: 23\n",
      "Episode 84: reward: -419.600, steps: 23\n",
      "Episode 85: reward: -419.600, steps: 23\n",
      "Episode 86: reward: -419.600, steps: 23\n",
      "Episode 87: reward: -419.600, steps: 23\n",
      "Episode 88: reward: -419.600, steps: 23\n",
      "Episode 89: reward: -419.600, steps: 23\n",
      "Episode 90: reward: -419.600, steps: 23\n",
      "Episode 91: reward: -419.600, steps: 23\n",
      "Episode 92: reward: -419.600, steps: 23\n",
      "Episode 93: reward: -419.600, steps: 23\n",
      "Episode 94: reward: -419.600, steps: 23\n",
      "Episode 95: reward: -419.600, steps: 23\n",
      "Episode 96: reward: -419.600, steps: 23\n",
      "Episode 97: reward: -419.600, steps: 23\n",
      "Episode 98: reward: -419.600, steps: 23\n",
      "Episode 99: reward: -419.600, steps: 23\n",
      "Episode 100: reward: -419.600, steps: 23\n",
      "-419.60000282526016\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c7f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
