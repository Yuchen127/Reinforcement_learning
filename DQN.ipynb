{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e620552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306060d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Peice array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start SOC\n",
    "        self.state = 20 #+ random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 sell\n",
    "        # 1 -1 = 0 hold\n",
    "        # 2 -1 = 1 buy \n",
    "        self.state += self.change * self.power  \n",
    "\n",
    "        \n",
    "        \n",
    "        if self.state == 100:\n",
    "            action = 1\n",
    "        if self.state == 0:\n",
    "            action = 0\n",
    "            \n",
    "        # Calculate reward    \n",
    "        # BUy\n",
    "        # Give plus reward is charge at low proce, and minus at high price\n",
    "        if action == 0:\n",
    "            self.change = 1\n",
    "            #if (self.max_state - self.state) < self.power:\n",
    "                #self.temp = self.max_state - self.state\n",
    "                #self.state = 100\n",
    "            if self.priceArray[self.currentIndex] < self.mid - 2:\n",
    "                reward = self.priceArray[self.currentIndex]\n",
    "            else:\n",
    "                reward = -1 * self.priceArray[self.currentIndex]\n",
    "                    \n",
    "        # Sell\n",
    "        elif action == 1:\n",
    "                #if self.state == 100 or self.priceArray[self.currentIndex] > 11 : \n",
    "                #    if self.state < self.power :\n",
    "                #        self.temp = self.state - 0\n",
    "                #        reward = 1 * self.priceArray[self.currentIndex] * (self.temp/self.power)\n",
    "                #        self.state = 0\n",
    "                #    else:\n",
    "                #        reward = 1 * self.priceArray[self.currentIndex]\n",
    "                #else:\n",
    "                #    self.change = 0\n",
    "                #    reward = -1\n",
    "            self.change = -1\n",
    "            #if self.state < self.power :\n",
    "                #self.temp = self.state - 0\n",
    "                #self.state = 0\n",
    "            if self.priceArray[self.currentIndex] > self.mid + 2:\n",
    "                reward = 1 * self.priceArray[self.currentIndex]\n",
    "            else:\n",
    "                reward = -1 * self.priceArray[self.currentIndex]\n",
    "                \n",
    "        # Hold        \n",
    "        else:\n",
    "            self.change = 0\n",
    "            if self.priceArray[self.currentIndex] > self.mid + 2 and self.priceArray[self.currentIndex] < self.mid - 2:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -5\n",
    "        \n",
    "        # Move to the next price\n",
    "        self.currentIndex += 1\n",
    "        \n",
    "        # Set Low boundary and Up boundary\n",
    "        #Set end state\n",
    "        if self.currentIndex == len(self.priceArray)-1: \n",
    "            done = True\n",
    "            if self.state == 100:\n",
    "                reward += 50\n",
    "            elif self.state >= 90:\n",
    "                reward += 30\n",
    "            elif self.state >= 80:\n",
    "                reward += 10\n",
    "                    \n",
    "            else:\n",
    "                reward = -100\n",
    "        elif self.state < 0 or self.state > 100:\n",
    "                done = True\n",
    "                reward = -100\n",
    "        else:\n",
    "            done = False       \n",
    "\n",
    "        \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 20 #+ random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbad91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuche\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = ChargeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca77928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.18931], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552762b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-191.72999912500381 State:20\n",
      "Episode:2 Score:-91.58000254631042 State:60\n",
      "Episode:3 Score:-75.86000162363052 State:20\n",
      "Episode:4 Score:-176.46000236272812 State:40\n",
      "Episode:5 Score:-223.25000166893005 State:60\n",
      "Episode:6 Score:-159.53999775648117 State:0\n",
      "Episode:7 Score:-105.49000066518784 State:40\n",
      "Episode:8 Score:-180.31000238656998 State:40\n",
      "Episode:9 Score:-160.30999940633774 State:40\n",
      "Episode:10 Score:-206.0600009560585 State:60\n",
      "Episode:11 Score:-190.05000162124634 State:80\n",
      "Episode:12 Score:-101.32000279426575 State:80\n",
      "Episode:13 Score:-162.5199996829033 State:0\n",
      "Episode:14 Score:-182.63000172376633 State:40\n",
      "Episode:15 Score:-141.6899978518486 State:40\n",
      "Episode:16 Score:-173.79999989271164 State:40\n",
      "Episode:17 Score:-152.3299970626831 State:60\n",
      "Episode:18 Score:-92.60999727249146 State:80\n",
      "Episode:19 Score:-129.41000074148178 State:20\n",
      "Episode:20 Score:-176.9099993109703 State:0\n",
      "Episode:21 Score:-332.65000051259995 State:60\n",
      "Episode:22 Score:-136.62000107765198 State:60\n",
      "Episode:23 Score:-46.790003061294556 State:100\n",
      "Episode:24 Score:-234.84999984502792 State:60\n",
      "Episode:25 Score:-223.14000231027603 State:20\n",
      "Episode:26 Score:-97.70999765396118 State:80\n",
      "Episode:27 Score:-166.26000171899796 State:60\n",
      "Episode:28 Score:-136.42000073194504 State:60\n",
      "Episode:29 Score:-219.33000260591507 State:40\n",
      "Episode:30 Score:-72.74000352621078 State:80\n",
      "Episode:31 Score:-83.31000202894211 State:80\n",
      "Episode:32 Score:-218.1199984550476 State:80\n",
      "Episode:33 Score:-224.44000101089478 State:60\n",
      "Episode:34 Score:-154.73000067472458 State:20\n",
      "Episode:35 Score:-128.3100044131279 State:20\n",
      "Episode:36 Score:-133.0400008559227 State:0\n",
      "Episode:37 Score:-104.67000365257263 State:100\n",
      "Episode:38 Score:-141.8799996972084 State:40\n",
      "Episode:39 Score:-150.33999913930893 State:80\n",
      "Episode:40 Score:-96.94000381231308 State:20\n",
      "Episode:41 Score:-171.41999858617783 State:60\n",
      "Episode:42 Score:-134.47999972105026 State:20\n",
      "Episode:43 Score:-124.71000081300735 State:40\n",
      "Episode:44 Score:-237.51000142097473 State:40\n",
      "Episode:45 Score:-30.67000240087509 State:100\n",
      "Episode:46 Score:-183.09000021219254 State:40\n",
      "Episode:47 Score:-196.28999996185303 State:20\n",
      "Episode:48 Score:-131.0699992775917 State:40\n",
      "Episode:49 Score:-233.55999904870987 State:40\n",
      "Episode:50 Score:-148.4699993133545 State:20\n",
      "Episode:51 Score:-100.55000132322311 State:80\n",
      "Episode:52 Score:-212.80000001192093 State:40\n",
      "Episode:53 Score:-259.44999998807907 State:20\n",
      "Episode:54 Score:-103.3799996972084 State:80\n",
      "Episode:55 Score:-41.44000202417374 State:100\n",
      "Episode:56 Score:-233.20000171661377 State:40\n",
      "Episode:57 Score:-116.44999974966049 State:80\n",
      "Episode:58 Score:-129.68000102043152 State:40\n",
      "Episode:59 Score:-57.4700031876564 State:100\n",
      "Episode:60 Score:-180.65000343322754 State:20\n",
      "Episode:61 Score:-85.07000070810318 State:80\n",
      "Episode:62 Score:-98.42999929189682 State:20\n",
      "Episode:63 Score:-89.72999852895737 State:60\n",
      "Episode:64 Score:-212.01999932527542 State:40\n",
      "Episode:65 Score:-183.87000185251236 State:20\n",
      "Episode:66 Score:-160.7799991965294 State:20\n",
      "Episode:67 Score:-25.56999808549881 State:80\n",
      "Episode:68 Score:-31.90999937057495 State:100\n",
      "Episode:69 Score:-193.19999879598618 State:40\n",
      "Episode:70 Score:-201.16000336408615 State:40\n",
      "Episode:71 Score:-141.88000059127808 State:60\n",
      "Episode:72 Score:-162.7499988079071 State:40\n",
      "Episode:73 Score:-66.71999698877335 State:80\n",
      "Episode:74 Score:-205.3000015616417 State:20\n",
      "Episode:75 Score:-109.16999727487564 State:40\n",
      "Episode:76 Score:-183.16000121831894 State:20\n",
      "Episode:77 Score:-156.319999396801 State:40\n",
      "Episode:78 Score:-147.18000334501266 State:80\n",
      "Episode:79 Score:-128.2299993634224 State:60\n",
      "Episode:80 Score:-197.96999925374985 State:60\n",
      "Episode:81 Score:-240.97000056505203 State:0\n",
      "Episode:82 Score:-35.00999873876572 State:80\n",
      "Episode:83 Score:-222.55000227689743 State:20\n",
      "Episode:84 Score:-186.0100023150444 State:40\n",
      "Episode:85 Score:-172.16000109910965 State:60\n",
      "Episode:86 Score:-84.31999945640564 State:20\n",
      "Episode:87 Score:-239.3600013256073 State:60\n",
      "Episode:88 Score:-180.48999792337418 State:20\n",
      "Episode:89 Score:-34.02000188827515 State:100\n",
      "Episode:90 Score:-108.06999725103378 State:40\n",
      "Episode:91 Score:-148.2099992632866 State:60\n",
      "Episode:92 Score:-188.07000249624252 State:60\n",
      "Episode:93 Score:-166.6799988746643 State:40\n",
      "Episode:94 Score:-160.6200025677681 State:40\n",
      "Episode:95 Score:-117.68000358343124 State:60\n",
      "Episode:96 Score:-125.73999810218811 State:40\n",
      "Episode:97 Score:-209.25999855995178 State:60\n",
      "Episode:98 Score:-68.29000145196915 State:80\n",
      "Episode:99 Score:-43.55000156164169 State:80\n",
      "Episode:100 Score:-152.4700021147728 State:100\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ccfc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4bc362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4592ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2eabdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c197b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply when 'Sequential' object has no attribute '_compile_time_distribution_strategy'\n",
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce45bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "558918c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 24)                48        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95934bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feafaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuche\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training_v1.py:2352: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  560/10000 [>.............................] - ETA: 1:09 - reward: -0.4553done, took 5.463 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22178222d00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mse']) #mean squared error mse, mean absloute error mae.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19df7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 0.130, steps: 23\n",
      "Episode 2: reward: 0.130, steps: 23\n",
      "Episode 3: reward: 0.130, steps: 23\n",
      "Episode 4: reward: 0.130, steps: 23\n",
      "Episode 5: reward: 0.130, steps: 23\n",
      "Episode 6: reward: 0.130, steps: 23\n",
      "Episode 7: reward: 0.130, steps: 23\n",
      "Episode 8: reward: 0.130, steps: 23\n",
      "Episode 9: reward: 0.130, steps: 23\n",
      "Episode 10: reward: 0.130, steps: 23\n",
      "Episode 11: reward: 0.130, steps: 23\n",
      "Episode 12: reward: 0.130, steps: 23\n",
      "Episode 13: reward: 0.130, steps: 23\n",
      "Episode 14: reward: 0.130, steps: 23\n",
      "Episode 15: reward: 0.130, steps: 23\n",
      "Episode 16: reward: 0.130, steps: 23\n",
      "Episode 17: reward: 0.130, steps: 23\n",
      "Episode 18: reward: 0.130, steps: 23\n",
      "Episode 19: reward: 0.130, steps: 23\n",
      "Episode 20: reward: 0.130, steps: 23\n",
      "Episode 21: reward: 0.130, steps: 23\n",
      "Episode 22: reward: 0.130, steps: 23\n",
      "Episode 23: reward: 0.130, steps: 23\n",
      "Episode 24: reward: 0.130, steps: 23\n",
      "Episode 25: reward: 0.130, steps: 23\n",
      "Episode 26: reward: 0.130, steps: 23\n",
      "Episode 27: reward: 0.130, steps: 23\n",
      "Episode 28: reward: 0.130, steps: 23\n",
      "Episode 29: reward: 0.130, steps: 23\n",
      "Episode 30: reward: 0.130, steps: 23\n",
      "Episode 31: reward: 0.130, steps: 23\n",
      "Episode 32: reward: 0.130, steps: 23\n",
      "Episode 33: reward: 0.130, steps: 23\n",
      "Episode 34: reward: 0.130, steps: 23\n",
      "Episode 35: reward: 0.130, steps: 23\n",
      "Episode 36: reward: 0.130, steps: 23\n",
      "Episode 37: reward: 0.130, steps: 23\n",
      "Episode 38: reward: 0.130, steps: 23\n",
      "Episode 39: reward: 0.130, steps: 23\n",
      "Episode 40: reward: 0.130, steps: 23\n",
      "Episode 41: reward: 0.130, steps: 23\n",
      "Episode 42: reward: 0.130, steps: 23\n",
      "Episode 43: reward: 0.130, steps: 23\n",
      "Episode 44: reward: 0.130, steps: 23\n",
      "Episode 45: reward: 0.130, steps: 23\n",
      "Episode 46: reward: 0.130, steps: 23\n",
      "Episode 47: reward: 0.130, steps: 23\n",
      "Episode 48: reward: 0.130, steps: 23\n",
      "Episode 49: reward: 0.130, steps: 23\n",
      "Episode 50: reward: 0.130, steps: 23\n",
      "Episode 51: reward: 0.130, steps: 23\n",
      "Episode 52: reward: 0.130, steps: 23\n",
      "Episode 53: reward: 0.130, steps: 23\n",
      "Episode 54: reward: 0.130, steps: 23\n",
      "Episode 55: reward: 0.130, steps: 23\n",
      "Episode 56: reward: 0.130, steps: 23\n",
      "Episode 57: reward: 0.130, steps: 23\n",
      "Episode 58: reward: 0.130, steps: 23\n",
      "Episode 59: reward: 0.130, steps: 23\n",
      "Episode 60: reward: 0.130, steps: 23\n",
      "Episode 61: reward: 0.130, steps: 23\n",
      "Episode 62: reward: 0.130, steps: 23\n",
      "Episode 63: reward: 0.130, steps: 23\n",
      "Episode 64: reward: 0.130, steps: 23\n",
      "Episode 65: reward: 0.130, steps: 23\n",
      "Episode 66: reward: 0.130, steps: 23\n",
      "Episode 67: reward: 0.130, steps: 23\n",
      "Episode 68: reward: 0.130, steps: 23\n",
      "Episode 69: reward: 0.130, steps: 23\n",
      "Episode 70: reward: 0.130, steps: 23\n",
      "Episode 71: reward: 0.130, steps: 23\n",
      "Episode 72: reward: 0.130, steps: 23\n",
      "Episode 73: reward: 0.130, steps: 23\n",
      "Episode 74: reward: 0.130, steps: 23\n",
      "Episode 75: reward: 0.130, steps: 23\n",
      "Episode 76: reward: 0.130, steps: 23\n",
      "Episode 77: reward: 0.130, steps: 23\n",
      "Episode 78: reward: 0.130, steps: 23\n",
      "Episode 79: reward: 0.130, steps: 23\n",
      "Episode 80: reward: 0.130, steps: 23\n",
      "Episode 81: reward: 0.130, steps: 23\n",
      "Episode 82: reward: 0.130, steps: 23\n",
      "Episode 83: reward: 0.130, steps: 23\n",
      "Episode 84: reward: 0.130, steps: 23\n",
      "Episode 85: reward: 0.130, steps: 23\n",
      "Episode 86: reward: 0.130, steps: 23\n",
      "Episode 87: reward: 0.130, steps: 23\n",
      "Episode 88: reward: 0.130, steps: 23\n",
      "Episode 89: reward: 0.130, steps: 23\n",
      "Episode 90: reward: 0.130, steps: 23\n",
      "Episode 91: reward: 0.130, steps: 23\n",
      "Episode 92: reward: 0.130, steps: 23\n",
      "Episode 93: reward: 0.130, steps: 23\n",
      "Episode 94: reward: 0.130, steps: 23\n",
      "Episode 95: reward: 0.130, steps: 23\n",
      "Episode 96: reward: 0.130, steps: 23\n",
      "Episode 97: reward: 0.130, steps: 23\n",
      "Episode 98: reward: 0.130, steps: 23\n",
      "Episode 99: reward: 0.130, steps: 23\n",
      "Episode 100: reward: 0.130, steps: 23\n",
      "0.12999624013900757\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c7f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
