{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9e620552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "306060d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChargeEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Observation box\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start SOC\n",
    "        self.state = 20 #+ random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 - buy\n",
    "        # 1 - sell\n",
    "        # 2 - hold \n",
    " \n",
    "\n",
    "        \n",
    "        # If state exceeds boundaries after the action due to power limit, set state to 100 or 0\n",
    "        # change action based on new state\n",
    "        if 100 < self.state < 120:\n",
    "            self.state = 100\n",
    "            action = 1\n",
    "        if -20 < self.state < 0:\n",
    "            self.state = 0\n",
    "            action = 0\n",
    "            \n",
    "        # Buy when state < 100 while approaching the end    \n",
    "        if (self.max_state - self.state) / self.power >= len(self.priceArray - 1) - self.currentIndex:\n",
    "            action = 0\n",
    "            \n",
    "        # Calculate reward    \n",
    "        # BUy\n",
    "        # Give - reward\n",
    "        if action == 0:\n",
    "            self.change = 1\n",
    "            #if (self.max_state - self.state) < self.power:\n",
    "                #self.temp = self.max_state - self.state\n",
    "                #self.state = 100\n",
    "            #if self.priceArray[self.currentIndex] < self.mid - 2:\n",
    "            #    reward = self.priceArray[self.currentIndex]\n",
    "            #else:\n",
    "            #    reward = -1 * self.priceArray[self.currentIndex]\n",
    "            reward = -self.priceArray[self.currentIndex] #* ((0.001 + self.state) / self.max_state)\n",
    "                    \n",
    "        # Sell\n",
    "        # Give + reward\n",
    "        elif action == 1:\n",
    "            self.change = -1\n",
    "            #if self.state < self.power :\n",
    "                #self.temp = self.state - 0\n",
    "                #self.state = 0\n",
    "            #if self.priceArray[self.currentIndex] > self.mid + 2:\n",
    "            #    reward = 1 * self.priceArray[self.currentIndex]\n",
    "            #else:\n",
    "            #    reward = -1 * self.priceArray[self.currentIndex]\n",
    "            reward = self.priceArray[self.currentIndex] #* ((0.001 + self.state) / self.max_state)\n",
    "                \n",
    "        # Hold        \n",
    "        else:\n",
    "            self.change = 0\n",
    "            reward = 0\n",
    "        \n",
    "        self.state += self.change * self.power \n",
    "            \n",
    "        # Move to the next price\n",
    "        self.currentIndex += 1\n",
    "        \n",
    "        # Set Low boundary and Up boundary\n",
    "        #Set end state\n",
    "        if self.currentIndex == len(self.priceArray)-1: \n",
    "            done = True\n",
    "            if self.state < 80:\n",
    "                reward = -1000\n",
    "        elif self.state <= -20 or self.state >= 120:\n",
    "                done = True\n",
    "                reward = -1000\n",
    "        else:\n",
    "            done = False       \n",
    "\n",
    "        \n",
    "\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 20 #+ random.randint(-3,3)\n",
    "        # Set max SOC\n",
    "        self.max_state = 100\n",
    "        # Set power\n",
    "        self.power = 20\n",
    "        # Set price data\n",
    "        self.priceArray = np.float32(np.array([36.22, 27.98, 4.6, 9.38, -0.43, 2.26, 4.02, 8.48, 18.73,\n",
    "                                               20.64, 22.99, 18.93, 16.67, 9.51, 3.59, 2.31, 2.78, 6.3, 14.18,\n",
    "                                               16.11, 21.42, 25.87, 30.71, 18.44]))\n",
    "        # Set current index\n",
    "        self.currentIndex = 0\n",
    "        # Set temp \n",
    "        self.temp = 0\n",
    "        self.change = 0\n",
    "        # Set median\n",
    "        self.mid = np.median(self.priceArray)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0fbad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChargeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ca77928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46.79038], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "552762b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-972.9677431232196 State:60\n",
      "Episode:2 Score:-1047.1819594575072 State:60\n",
      "Episode:3 Score:-47.82798325872123 State:80\n",
      "Episode:4 Score:-62.1081006480807 State:80\n",
      "Episode:5 Score:-34.289843985569476 State:80\n",
      "Episode:6 Score:-96.62173851014794 State:80\n",
      "Episode:7 Score:-43.4697852816087 State:80\n",
      "Episode:8 Score:-31.849850562810293 State:80\n",
      "Episode:9 Score:-980.3459290659475 State:60\n",
      "Episode:10 Score:-75.60202133058011 State:80\n",
      "Episode:11 Score:-112.7758136752528 State:80\n",
      "Episode:12 Score:-1076.5818218628758 State:60\n",
      "Episode:13 Score:-1004.3857344701737 State:60\n",
      "Episode:14 Score:-24.397916355524657 State:80\n",
      "Episode:15 Score:-973.7960962872053 State:60\n",
      "Episode:16 Score:-75.5458184686619 State:80\n",
      "Episode:17 Score:-1020.7761140931725 State:60\n",
      "Episode:18 Score:-132.267766261639 State:80\n",
      "Episode:19 Score:-1039.902113788764 State:60\n",
      "Episode:20 Score:-75.77987030708076 State:80\n",
      "Episode:21 Score:-1059.8876611610276 State:60\n",
      "Episode:22 Score:-70.20587352138462 State:80\n",
      "Episode:23 Score:34.24813241975725 State:80\n",
      "Episode:24 Score:-57.7297968468076 State:80\n",
      "Episode:25 Score:-52.98394121259868 State:80\n",
      "Episode:26 Score:-84.29590279189885 State:80\n",
      "Episode:27 Score:-1023.6961204303116 State:60\n",
      "Episode:28 Score:-67.38359272642673 State:80\n",
      "Episode:29 Score:-78.98187371580124 State:80\n",
      "Episode:30 Score:-53.28183998466551 State:80\n",
      "Episode:31 Score:-66.07990091838062 State:100\n",
      "Episode:32 Score:-59.681960548910496 State:80\n",
      "Episode:33 Score:-117.80784367716493 State:80\n",
      "Episode:34 Score:-985.4900268100881 State:60\n",
      "Episode:35 Score:-89.7037679465896 State:80\n",
      "Episode:36 Score:-1074.5458684356172 State:60\n",
      "Episode:37 Score:-22.375909737390877 State:80\n",
      "Episode:38 Score:5.687734617492559 State:80\n",
      "Episode:39 Score:-29.17399609890699 State:80\n",
      "Episode:40 Score:-18.54983526157915 State:80\n",
      "Episode:41 Score:-1020.3618869633538 State:60\n",
      "Episode:42 Score:-1027.6660529529995 State:60\n",
      "Episode:43 Score:-12.967920874791744 State:80\n",
      "Episode:44 Score:-46.49599649837255 State:100\n",
      "Episode:45 Score:-35.47401371880054 State:100\n",
      "Episode:46 Score:-79.49385155469955 State:80\n",
      "Episode:47 Score:-52.477911442173124 State:80\n",
      "Episode:48 Score:-67.06204321906328 State:80\n",
      "Episode:49 Score:-52.06622269400359 State:80\n",
      "Episode:50 Score:-80.89594589128018 State:80\n",
      "Episode:51 Score:-106.09371073813975 State:80\n",
      "Episode:52 Score:-72.38405698153555 State:100\n",
      "Episode:53 Score:-72.03772789155542 State:80\n",
      "Episode:54 Score:-40.73576942451537 State:80\n",
      "Episode:55 Score:-992.3178639264911 State:60\n",
      "Episode:56 Score:37.02993684427916 State:80\n",
      "Episode:57 Score:-94.0316209250617 State:80\n",
      "Episode:58 Score:-9.084153776841163 State:80\n",
      "Episode:59 Score:-74.73357568147004 State:80\n",
      "Episode:60 Score:-103.31380288733064 State:80\n",
      "Episode:61 Score:25.710070118324168 State:80\n",
      "Episode:62 Score:-89.95200973323108 State:80\n",
      "Episode:63 Score:-1069.0118343283934 State:60\n",
      "Episode:64 Score:-1076.3655967238205 State:60\n",
      "Episode:65 Score:-55.34183248576701 State:80\n",
      "Episode:66 Score:-66.91594435626209 State:80\n",
      "Episode:67 Score:10.624069413494468 State:80\n",
      "Episode:68 Score:-1026.8537722836375 State:60\n",
      "Episode:69 Score:-27.002255600075124 State:100\n",
      "Episode:70 Score:-1070.3777083362222 State:60\n",
      "Episode:71 Score:-965.8960588950991 State:60\n",
      "Episode:72 Score:-949.9260417128181 State:60\n",
      "Episode:73 Score:-39.39396870980083 State:80\n",
      "Episode:74 Score:-976.2239183546883 State:60\n",
      "Episode:75 Score:-85.99586390443504 State:80\n",
      "Episode:76 Score:-115.28793223725856 State:100\n",
      "Episode:77 Score:-1021.3199420179844 State:60\n",
      "Episode:78 Score:-91.2318807179004 State:80\n",
      "Episode:79 Score:-3.774221330792905 State:80\n",
      "Episode:80 Score:5.338139333987833 State:80\n",
      "Episode:81 Score:-78.76987427499235 State:80\n",
      "Episode:82 Score:-1050.3357439352137 State:60\n",
      "Episode:83 Score:-75.50585258489191 State:80\n",
      "Episode:84 Score:-79.59796936213732 State:80\n",
      "Episode:85 Score:-1026.6239346115797 State:60\n",
      "Episode:86 Score:-39.949915942988994 State:80\n",
      "Episode:87 Score:-66.68585512126208 State:80\n",
      "Episode:88 Score:-1052.2977363245082 State:60\n",
      "Episode:89 Score:-84.14806898670852 State:80\n",
      "Episode:90 Score:-1015.0838190444111 State:60\n",
      "Episode:91 Score:-86.80804084999264 State:80\n",
      "Episode:92 Score:-1036.8638450224037 State:60\n",
      "Episode:93 Score:-66.89992389931739 State:80\n",
      "Episode:94 Score:-47.452027834466094 State:80\n",
      "Episode:95 Score:-51.638134693531384 State:80\n",
      "Episode:96 Score:-95.37979097737491 State:80\n",
      "Episode:97 Score:0.9741228771108439 State:80\n",
      "Episode:98 Score:-974.9400201546126 State:60\n",
      "Episode:99 Score:-56.449741564226734 State:80\n",
      "Episode:100 Score:-59.6100252506876 State:80\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} State:{}'.format(episode, score, n_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e4bc362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4592ef26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d2eabdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c197b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply when 'Sequential' object has no attribute '_compile_time_distribution_strategy'\n",
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ce45bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "558918c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 24)                48        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "feafaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dfd500cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -6.3958\n",
      "434 episodes - episode_reward: -147.369 [-1090.880, -17.816] - loss: 2560.557 - mse: 4391.467 - mean_q: -34.112\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: -3.3930\n",
      "435 episodes - episode_reward: -78.000 [-78.000, -78.000] - loss: 1250.667 - mse: 4645.794 - mean_q: -41.371\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -3.3930\n",
      "435 episodes - episode_reward: -78.000 [-78.000, -78.000] - loss: 725.002 - mse: 3930.915 - mean_q: -43.060\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: -3.3920\n",
      "435 episodes - episode_reward: -77.977 [-97.384, -57.042] - loss: 445.725 - mse: 3735.909 - mean_q: -51.464\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "  315/10000 [..............................] - ETA: 1:38 - reward: -6.3318done, took 401.277 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2557e5d1fa0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4), metrics=['mse']) #mean squared error mse, mean absloute error mae.\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "19df7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -78.000, steps: 23\n",
      "Episode 2: reward: -78.000, steps: 23\n",
      "Episode 3: reward: -78.000, steps: 23\n",
      "Episode 4: reward: -78.000, steps: 23\n",
      "Episode 5: reward: -78.000, steps: 23\n",
      "Episode 6: reward: -78.000, steps: 23\n",
      "Episode 7: reward: -78.000, steps: 23\n",
      "Episode 8: reward: -78.000, steps: 23\n",
      "Episode 9: reward: -78.000, steps: 23\n",
      "Episode 10: reward: -78.000, steps: 23\n",
      "Episode 11: reward: -78.000, steps: 23\n",
      "Episode 12: reward: -78.000, steps: 23\n",
      "Episode 13: reward: -78.000, steps: 23\n",
      "Episode 14: reward: -78.000, steps: 23\n",
      "Episode 15: reward: -78.000, steps: 23\n",
      "Episode 16: reward: -78.000, steps: 23\n",
      "Episode 17: reward: -78.000, steps: 23\n",
      "Episode 18: reward: -78.000, steps: 23\n",
      "Episode 19: reward: -78.000, steps: 23\n",
      "Episode 20: reward: -78.000, steps: 23\n",
      "Episode 21: reward: -78.000, steps: 23\n",
      "Episode 22: reward: -78.000, steps: 23\n",
      "Episode 23: reward: -78.000, steps: 23\n",
      "Episode 24: reward: -78.000, steps: 23\n",
      "Episode 25: reward: -78.000, steps: 23\n",
      "Episode 26: reward: -78.000, steps: 23\n",
      "Episode 27: reward: -78.000, steps: 23\n",
      "Episode 28: reward: -78.000, steps: 23\n",
      "Episode 29: reward: -78.000, steps: 23\n",
      "Episode 30: reward: -78.000, steps: 23\n",
      "Episode 31: reward: -78.000, steps: 23\n",
      "Episode 32: reward: -78.000, steps: 23\n",
      "Episode 33: reward: -78.000, steps: 23\n",
      "Episode 34: reward: -78.000, steps: 23\n",
      "Episode 35: reward: -78.000, steps: 23\n",
      "Episode 36: reward: -78.000, steps: 23\n",
      "Episode 37: reward: -78.000, steps: 23\n",
      "Episode 38: reward: -78.000, steps: 23\n",
      "Episode 39: reward: -78.000, steps: 23\n",
      "Episode 40: reward: -78.000, steps: 23\n",
      "Episode 41: reward: -78.000, steps: 23\n",
      "Episode 42: reward: -78.000, steps: 23\n",
      "Episode 43: reward: -78.000, steps: 23\n",
      "Episode 44: reward: -78.000, steps: 23\n",
      "Episode 45: reward: -78.000, steps: 23\n",
      "Episode 46: reward: -78.000, steps: 23\n",
      "Episode 47: reward: -78.000, steps: 23\n",
      "Episode 48: reward: -78.000, steps: 23\n",
      "Episode 49: reward: -78.000, steps: 23\n",
      "Episode 50: reward: -78.000, steps: 23\n",
      "Episode 51: reward: -78.000, steps: 23\n",
      "Episode 52: reward: -78.000, steps: 23\n",
      "Episode 53: reward: -78.000, steps: 23\n",
      "Episode 54: reward: -78.000, steps: 23\n",
      "Episode 55: reward: -78.000, steps: 23\n",
      "Episode 56: reward: -78.000, steps: 23\n",
      "Episode 57: reward: -78.000, steps: 23\n",
      "Episode 58: reward: -78.000, steps: 23\n",
      "Episode 59: reward: -78.000, steps: 23\n",
      "Episode 60: reward: -78.000, steps: 23\n",
      "Episode 61: reward: -78.000, steps: 23\n",
      "Episode 62: reward: -78.000, steps: 23\n",
      "Episode 63: reward: -78.000, steps: 23\n",
      "Episode 64: reward: -78.000, steps: 23\n",
      "Episode 65: reward: -78.000, steps: 23\n",
      "Episode 66: reward: -78.000, steps: 23\n",
      "Episode 67: reward: -78.000, steps: 23\n",
      "Episode 68: reward: -78.000, steps: 23\n",
      "Episode 69: reward: -78.000, steps: 23\n",
      "Episode 70: reward: -78.000, steps: 23\n",
      "Episode 71: reward: -78.000, steps: 23\n",
      "Episode 72: reward: -78.000, steps: 23\n",
      "Episode 73: reward: -78.000, steps: 23\n",
      "Episode 74: reward: -78.000, steps: 23\n",
      "Episode 75: reward: -78.000, steps: 23\n",
      "Episode 76: reward: -78.000, steps: 23\n",
      "Episode 77: reward: -78.000, steps: 23\n",
      "Episode 78: reward: -78.000, steps: 23\n",
      "Episode 79: reward: -78.000, steps: 23\n",
      "Episode 80: reward: -78.000, steps: 23\n",
      "Episode 81: reward: -78.000, steps: 23\n",
      "Episode 82: reward: -78.000, steps: 23\n",
      "Episode 83: reward: -78.000, steps: 23\n",
      "Episode 84: reward: -78.000, steps: 23\n",
      "Episode 85: reward: -78.000, steps: 23\n",
      "Episode 86: reward: -78.000, steps: 23\n",
      "Episode 87: reward: -78.000, steps: 23\n",
      "Episode 88: reward: -78.000, steps: 23\n",
      "Episode 89: reward: -78.000, steps: 23\n",
      "Episode 90: reward: -78.000, steps: 23\n",
      "Episode 91: reward: -78.000, steps: 23\n",
      "Episode 92: reward: -78.000, steps: 23\n",
      "Episode 93: reward: -78.000, steps: 23\n",
      "Episode 94: reward: -78.000, steps: 23\n",
      "Episode 95: reward: -78.000, steps: 23\n",
      "Episode 96: reward: -78.000, steps: 23\n",
      "Episode 97: reward: -78.000, steps: 23\n",
      "Episode 98: reward: -78.000, steps: 23\n",
      "Episode 99: reward: -78.000, steps: 23\n",
      "Episode 100: reward: -78.000, steps: 23\n",
      "-78.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c7f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
